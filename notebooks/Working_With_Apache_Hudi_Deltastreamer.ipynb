{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Apache Hudi Deltastreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Apache Hudi Deltastreamer\n",
    "HoodieDeltaStreamer utility is part of hudi-utilities-bundle that provides a way to ingest data from sources such as DFS or Kafka.\n",
    "\n",
    "In this notebook, you will learn to use DeltaStreamer Utility to bulk insert data into a Hudi Dataset as a Copy on Write(CoW) table (and Merge on Read [MOR] table) , apply change data capture (CDC) to the tables and query the Hudi dataset using Hive. \n",
    "\n",
    "We will run queries in hudi-cli to verify the tables and subsequent updates are incorporated into our datalake on Amazon S3\n",
    "\n",
    "Let's get started !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Faker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Profile Generator\n",
    "\n",
    "Fake profile generator uses Python's Faker [https://faker.readthedocs.io/en/master/index.html] library. Let's define a method to generate a number of random person profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import boto3\n",
    "import io\n",
    "from faker import Faker\n",
    "from faker.providers import date_time, credit_card\n",
    "from json import dumps\n",
    "\n",
    "\n",
    "# Intialize Faker library and S3 client\n",
    "fake = Faker() \n",
    "fake.add_provider(date_time)\n",
    "fake.add_provider(credit_card)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Write the fake profile data to a S3 bucket\n",
    "# Replace with your own bucket\n",
    "s3_bucket = \"mybucket\"\n",
    "s3_load_prefix = 'hudi-ds/inputdata/'\n",
    "s3_update_prefix = 'hudi-ds/updates/'\n",
    "\n",
    "# Number of records in each file and number of files\n",
    "# Adjust per your need - this produces 40MB files\n",
    "#num_records = 150000\n",
    "#num_files = 50\n",
    "\n",
    "num_records = 7\n",
    "num_files = 3\n",
    "\n",
    "def generate_bulk_data():\n",
    "    '''\n",
    "    Generates bulk profile data\n",
    "    '''\n",
    "    # Generate number of files equivalent to num_files\n",
    "    for i in range (num_files):\n",
    "        fake_profile_data = fake_profile_generator(num_records, fake)\n",
    "        fakeIO = StringIO()\n",
    "        filename = 'profile_' + str(i + 1) + '.json'\n",
    "        s3key = s3_load_prefix + filename \n",
    "\n",
    "        fakeIO.write(str(''.join(dumps_lines(fake_profile_data))))\n",
    "\n",
    "        s3object = s3.Object(s3_bucket, s3key)\n",
    "        s3object.put(Body=(bytes(fakeIO.getvalue().encode('UTF-8'))))\n",
    "        fakeIO.close()\n",
    "\n",
    "def generate_updates():\n",
    "    '''\n",
    "    Generates updates for the profiles\n",
    "    '''\n",
    "    #\n",
    "    # We will make updates to records in randomly picked files\n",
    "    #\n",
    "    random_file_list = []\n",
    "    \n",
    "    for i in range (1, num_files):\n",
    "        random_file_list.append('profile_' + str(i) + '.json')\n",
    "    \n",
    "    for f in random_file_list:\n",
    "        print(f)\n",
    "        s3key = s3_load_prefix + f\n",
    "        obj = s3.Object(s3_bucket, s3key)\n",
    "        profile_data = obj.get()['Body'].read().decode('utf-8')\n",
    "        \n",
    "        #s3_profile_list = json.loads(profile_data)\n",
    "        stringIO_data = io.StringIO(profile_data)\n",
    "        data = stringIO_data.readlines()\n",
    "\n",
    "        #Its time to use json module now.\n",
    "        json_data = list(map(json.loads, data))\n",
    "\n",
    "        fakeIO = StringIO()\n",
    "        s3key = s3_update_prefix + f\n",
    "        fake_profile_data = []\n",
    "        \n",
    "        for rec in json_data:\n",
    "            # Let's generate a new address\n",
    "            print (\"old address: \" + rec['street_address'])\n",
    "            rec['street_address'] = fake.address()\n",
    "            print (\"new address: \" + rec['street_address'])\n",
    "            fake_profile_data.append(rec)\n",
    "            \n",
    "        fakeIO.write(str(''.join(dumps_lines(fake_profile_data))))\n",
    "        s3object = s3.Object(s3_bucket, s3key)\n",
    "        s3object.put(Body=(bytes(fakeIO.getvalue().encode('UTF-8'))))\n",
    "        fakeIO.close()\n",
    "\n",
    "def fake_profile_generator(length, fake, new_address=\"\"):\n",
    "    \"\"\"\n",
    "    Generates fake profiles\n",
    "    \"\"\"\n",
    "    for x in range (length):       \n",
    "        yield {'Name': fake.name(),\n",
    "               'phone': fake.phone_number(),\n",
    "               'job': fake.job(),\n",
    "               'company': fake.company(),\n",
    "               'ssn': fake.ssn(),\n",
    "               'street_address': (new_address if new_address else fake.address()),\n",
    "               'dob': (fake.date_of_birth(tzinfo=None, minimum_age=21, maximum_age=105).isoformat()),\n",
    "               'email': fake.email(),\n",
    "               'ts': (fake.date_time_between(start_date='-10y', end_date='now', tzinfo=None).isoformat()),\n",
    "               'credit_card': fake.credit_card_number(),\n",
    "               'record_id': fake.pyint(),\n",
    "               'id': fake.uuid4()}\n",
    "        \n",
    "def dumps_lines(objs):\n",
    "    for obj in objs:\n",
    "        yield json.dumps(obj, separators=(',',':')) + '\\n'   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the data generator\n",
    "\n",
    "Following code kicks off the fake data generator to produce files each with certain records (configurable) in JSON format. The files are written to a specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bulk_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Hudi Libraries on the EMR Cluster and create Hive table\n",
    "\n",
    "0. For the following steps to work, you should have launched the EMR cluster with appropriate permissions set for **Systems Manager Session Manager** \n",
    "1. From the AWS Console, type SSM in the search box and navigate to the **Amazon System Manager console**\n",
    "2. On the left hand side, select **Session Manager** from **Instances and Nodes** section\n",
    "3. Click on the start session and you should see two EC2 instances listed \n",
    "4. Select instance-id of the **EMR's Master** Node and click on **Start session**\n",
    "5. From the terminal type the following to change to user *ec2-user*\n",
    " \n",
    "```bash\n",
    "sh-4.2$ sudo su hadoop\n",
    "hadoop@ip-10-0-2-73 /]$ cd\n",
    "[hadoop@ip-10-0-2-73 ~]$ hdfs dfs -mkdir -p /apps/hudi/lib\n",
    "[hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar\n",
    "hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-utilities-bundle.jar /apps/hudi/lib/hudi-utilities-bundle.jar\n",
    "[hadoop@ip-10-0-2-73 ~]$ hdfs dfs -ls /apps/hudi/lib/\n",
    "Found 3 items\n",
    "-rw-r--r--   2 hadoop hadoop   20967361 2020-03-10 04:10 /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "-rw-r--r--   2 hadoop hadoop   39051878 2020-03-10 04:27 /apps/hudi/lib/hudi-utilities-bundle.jar\n",
    "-rw-r--r--   2 hadoop hadoop     187458 2020-03-10 04:11 /apps/hudi/lib/spark-avro.jar\n",
    "```\n",
    "\n",
    "6. Create an external table from Hive as follows\n",
    "\n",
    "```\n",
    "hive> CREATE EXTERNAL TABLE profile_test_cow(Name string,\n",
    "    > phone string,\n",
    "    > job string,\n",
    "    > company string,\n",
    "    > ssn string,\n",
    "    > street_address string,\n",
    "    > dob string,\n",
    "    > email string,\n",
    "    > ts string,\n",
    "    > credit_card string,\n",
    "    > record_id int,\n",
    "    > id string)\n",
    "    > ROW FORMAT SERDE\n",
    "    > 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "    > STORED AS INPUTFORMAT\n",
    "    > 'com.uber.hoodie.hadoop.HoodieInputFormat'\n",
    "    > OUTPUTFORMAT\n",
    "    > 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
    "    > LOCATION\n",
    "    > 's3://<my-bucket>/hudi-ds/output/profile-test-out/';\n",
    "OK\n",
    "Time taken: 3.575 seconds\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeltaStreamer to write a Copy on Write (COW) table\n",
    "\n",
    "We will now run the DeltaStreamer utility as an EMR Step to write the above JSON formatted data into a Hudi dataset. To do that, we will need the following:\n",
    "\n",
    "* Properties file on localfs or dfs, with configurations for Hudi client, schema provider, key generator and data source \n",
    "* Schema file for source dataset\n",
    "* Schema file for target dataset\n",
    "\n",
    "To run DeltaStreamer\n",
    "\n",
    "```\n",
    "aws emr add-steps --cluster-id j-XXXXZOOXXXXX --steps Type=Spark,Name=\"Deltastreamer COW\",ActionOnFailure=CONTINUE,Args=[--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://<my-bucket>/config/json-deltastreamer.properties,--storage-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://<my-bucket>/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Replace the following values in the above command in the text editor\n",
    "\n",
    "1. --cluster-id with the value you got from previous step\n",
    "2. For --props value replace xxxx part in hudi-workshop-xxxx with the S3 bucket name \n",
    "3. For -- target-base-path value with the S3 bucket name\n",
    "4. After replacing the values, copy the entire commmand and run it in the next cell\n",
    "5. If the values are replaced correctly, you should see a step id displayed as the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws emr add-steps --cluster-id j-1QXXXXXXXXX --steps Type=Spark,Name=\"Deltastreamer Profile COW\",ActionOnFailure=CONTINUE,Args=[--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://aws-test123/hudi-ds/config/json-deltastreamer-upserts.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://aws-test123/hudi-ds/output/profile-test-out,--target-table,profile_test_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,BULK_INSERT] --region us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Hudi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the Hudi dataset you can do one of the following\n",
    "\n",
    "- Navigate to the another sparkmagic notebook and run queries in Spark \n",
    "- SSH to the master node (you can also SSM if you launched your cluster with SSM permissions) and run queries using Hive/Presto\n",
    "- Head to the Hue console on Amazon EMR and run queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run updates now on the fake profile data generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_updates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeltaStreamer to apply updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the Deltastreamer again to run upserts using the updates generated in the previous step.\n",
    "\n",
    "```\n",
    "\n",
    "! aws emr add-steps --cluster-id j-XXXXXXX --steps Type=Spark,Name=\"Deltastreamer Profile Upserts\",ActionOnFailure=CONTINUE,Args=[--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://<my-bucket>/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://<my-bucket>/hudi-ds/output/profile-test15-out,--target-table,profile_test15_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws emr add-steps --cluster-id j-1XXXXXXXXX --steps Type=Spark,Name=\"Deltastreamer Profile Upserts\",ActionOnFailure=CONTINUE,Args=[--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://<my-bucket>/hudi-ds/config/json-deltastreamer-upserts.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://<my-bucket>/hudi-ds/output/profile-test-out,--target-table,profile_test_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the updated Hudi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the Hudi dataset you can do one of the following\n",
    "\n",
    "- Navigate to the another sparkmagic notebook and run queries in Spark \n",
    "- SSH to the master node (you can also SSM if you launched your cluster with SSM permissions) and run queries using Hive/Presto\n",
    "- Head to the Hue console on Amazon EMR and run queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
